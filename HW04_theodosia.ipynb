{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8136941",
   "metadata": {},
   "source": [
    "# HomeWork 04\n",
    "\n",
    "<font size=1>\n",
    "    Nikolaos Vasilas & Elias Kyritsis, 2024. All rights reserved.\n",
    "</font>  \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"margin-top: 20px\">\n",
    "       \n",
    "**Exercise 1: Support Vector Machine Classification**\n",
    "    <br><br>\n",
    "    Following this week's lecture, let's use the Support Vector Machine and train an algorithm on real astronomical data. In modern astrophysics, the continuous flow of data and the need to analyze it accurately has made machine learning algorithms very helpful for observational and theoretical astronomers.<br><br>\n",
    "\n",
    "**Objective:** \n",
    "<br>\n",
    "Stellar classification relies on a star's spectral data to sort stars into distinct categories. The current system, called the Morgan–Keenan (MK) classification, is based on the earlier Hertzsprung–Russell (HR) classification and uses chromaticity along with Roman numerals to indicate a star’s size. In this exercise, we’ll be using the Absolute and Apparent Magnitude, B-V Color Index, Parallax Distance, Surface Temperature, and Luminosity of stars to distinguish between Giants and Dwarfs.\n",
    "    \n",
    "    \n",
    "> Keep in mind that:\n",
    "    >\n",
    "> - The Absolute magnitude can be calculated as:\n",
    "$$\n",
    "\\begin{equation}\n",
    "M = m + 5(\\log_{10}(p) +1)  \\tag{Eq. 1}\n",
    "\\end{equation}\n",
    "$$\n",
    "<br>\n",
    "    > - The Surface Temperature in Kelvin is given by the formula:\n",
    "    $$\n",
    "\\begin{equation}\n",
    "T = \\frac{4600}{0.92 \\cdot (B - V) + 1.7} + \\frac{4600}{0.92 \\cdot (B - V) + 0.62}\\tag{Eq. 2}\n",
    "\\end{equation}\n",
    "$$\n",
    "<br>\n",
    "    > - The luminosity of a star:\n",
    " $$\n",
    "\\begin{equation}\n",
    "\\frac{L}{L_\\odot} = 10^{0.4 (4.85 - M)}\\tag{Eq. 3}\n",
    "\\end{equation}\n",
    "$$\n",
    "    > <br><br>\n",
    "> Where \\( $M$ \\) is the Absolute Magnitude, \\( $m$ \\) is the Apparent Magnitude, \\( $p$ \\) is the parallax (distance between the Sun and the star), \\( $T$ \\) is the Surface Temperature, and \\( $L$ \\) is the Luminosity of stars. \n",
    "\n",
    "The dataset contains 3180 rows and 7 columns (features). The features included in the dataset are:\n",
    "     \n",
    "    Vmag (Apparent Magnitude)\n",
    "    Plx (Parallax Distance)\n",
    "    B-V (B-V Color Index)\n",
    "    Amag (Absolute Magnitude)\n",
    "    Temp/Teff (K) (Surface Temperature)\n",
    "    Luminosity (W) (Luminosity of stars)\n",
    "    TargetClass (The target classification label)\n",
    "<br>\n",
    "    \n",
    "**Tasks:**\n",
    "<br>\n",
    "\n",
    "- **Ex.1.1**: Load the data from the file ```HW04_data.csv```. \n",
    "    - Using the ```seaborn``` library plot a `pairplot` for the features, color-coding the instances accorind to the class each instance bellongs.\n",
    "    - Plot a histogram of the target labels. Comment in one sentence whether there is a posibility that whatever model you will try to train will encounter problems.\n",
    "    - Add a new column named \"class_numeric\", where you will replace each class with numerical values, e.g., 'Dwarf' $\\leftrightarrow$ 0,and 'Giant' $\\leftrightarrow$ 1.<br>\n",
    "<br>\n",
    "    \n",
    "- **Ex.1.2**: Constuct an SVM model for dinsiguishing between 'Dwarf', and 'Giant'. Use as features all the feature of the dataset. In addition, use the default SVM hyperparameters. For this example use a simple train-test protocol (no validation, no CV):\n",
    "    - Split data in _train_ and _test_\n",
    "    - Fit on _train_\n",
    "    - Assess performance on _test_ using the _accuracy_ metric\n",
    "    - Print the accuracy of your model.<br>\n",
    "<br>\n",
    "    \n",
    "- **Ex.1.3**: A standard technique for the oprimization of the hyperparameters is to use the Grid Search method. Grid Search CV it is a cross-validation technique for finding the optimal hyperparameter values from a given set of hyperparameters in a grid. Use ```sklearn.model_selection.GridSearchCV```, to tune the **_C_** hyperparameter **AND** the **kernel** hyperparameter (and any other hyperparameter you wish) of the SVM applied on your data.\n",
    "    - For the **C** and **kernel** hyperparameters try the value ranges **C = [1,5,10]** and **kernel = ['linear', 'poly', 'rbf']**. Set the number of folds to 5 (i.e., `cv=5`). Print the best score and the best fit parameters that the GridSearchCV returns.\n",
    "    - By using the optimal hyperparameters combination, evaluate the performance of your best model on a hold-out test that you kept out in the begining of **Ex.1.3** (separated from your a master training set). Print the accuracy of your best model, the final classification report and the confusion matrix of your best model. \n",
    "    - What do you notice, by comparing the accuracy scores of Ex.1.2 and Ex.1.3 ? Comment in one-two sentences.\n",
    "<br><br>\n",
    "\n",
    "- **[Bonus] Ex 1.4**: <br>\n",
    "    Run the same protocol as in **Ex.1.3** but now train for the features, 'Amag', 'Temp/Teff (K)', and 'Luminosity (W)'. Compare the accuracy from that of **Ex.1.3**: do you expect that result? \n",
    "<br>\n",
    "    \n",
    "- **[Bonus] Ex 1.5**: <br>\n",
    "   Plot the decision function of the best models of **Ex.1.3**, and **Ex.1.4** in the (Surface Temperature, Luminosity) space.\n",
    "\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- The documentation of SVM algorithm is [here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "- The documentation of ```sklearn.model_selection.GridSearchCV``` is [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "- Load the data with ```pandas.read_csv```. See documentation [here](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) \n",
    "- For the third step of **Ex.1.1** see the documentation [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.map.html)\n",
    "- For **Ex 1.5** use:\n",
    "    ```plt.xscale('log')``` and \n",
    "     ```plt.yscale('log')```\n",
    "- Don't forget to nomalize before running SVM $-$ use the `StandardScaler` (documentation [here](https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.StandardScaler.html))\n",
    "- Don't forget to have fun!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b11f752",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Vmag    Plx    B-V TargetClass       Amag  Temp/Teff (K)  \\\n",
      "0     5.99  13.73  1.318       Dwarf  16.678353    4089.516341   \n",
      "1     8.70   2.31 -0.045       Dwarf  15.518060   10723.648049   \n",
      "2     5.77   5.50  0.855       Dwarf  14.471813    5120.212718   \n",
      "3     6.72   5.26 -0.015       Giant  15.324929   10316.282219   \n",
      "4     8.76  13.44  0.584       Giant  19.401996    6030.905633   \n",
      "...    ...    ...    ...         ...        ...            ...   \n",
      "3175  7.79  12.92  0.772       Dwarf  18.346313    5366.546245   \n",
      "3176  7.29   3.26  1.786       Dwarf  14.856088    3408.552355   \n",
      "3177  8.29   6.38  0.408       Giant  17.314103    6837.926421   \n",
      "3178  6.11   2.42  1.664       Dwarf  13.029077    3562.420235   \n",
      "3179  8.81   1.87  1.176       Dwarf  15.169208    4356.363995   \n",
      "\n",
      "      Luminosity (W)  class_numeric  \n",
      "0       6.976392e+21              0  \n",
      "1       2.031178e+22              0  \n",
      "2       5.324104e+22              0  \n",
      "3       2.426613e+22              1  \n",
      "4       5.677712e+20              1  \n",
      "...              ...            ...  \n",
      "3175    1.501229e+21              0  \n",
      "3176    3.737117e+22              0  \n",
      "3177    3.884459e+21              1  \n",
      "3178    2.010672e+23              0  \n",
      "3179    2.800843e+22              0  \n",
      "\n",
      "[3180 rows x 8 columns]\n",
      "Accuracy score: 0.9135220125786163\n",
      "Best score: 0.8985845335148431\n",
      "Best parameters: {'C': 1, 'kernel': 'rbf'}\n",
      "Accuracy score for the best model: 0.9135220125786163\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "scale = StandardScaler()\n",
    "\n",
    "df = pd. read_csv('C:/Users/souli/OneDrive/Desktop/ML/HW04_data.csv')\n",
    "\n",
    "#normalize\n",
    "\n",
    "df['class_numeric']=df['TargetClass']\n",
    "df['class_numeric'].replace({'Dwarf': 0,'Giant':1}, inplace=True) \n",
    "\n",
    "X=df[['Vmag', 'Plx','B-V','Amag','Temp/Teff (K)','Luminosity (W)']] #2d\n",
    "columns=pd.DataFrame(['Vmag', 'Plx','B-V','Amag','Temp/Teff (K)','Luminosity (W)'])\n",
    "print(df)\n",
    "y=np.array(df['class_numeric'])\n",
    "\n",
    "class Klasi():\n",
    "    def __init__(self,X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.df = pd.DataFrame(X, columns=columns) \n",
    "        self.df['class_numeric'] = y\n",
    "    def splitdata(self):\n",
    "        X_train,X_test,y_train,y_test= train_test_split(self.X, self.y, test_size=0.2,random_state=2)\n",
    "        return(X_train,X_test,y_train,y_test) \n",
    "    def plotdata(self):\n",
    "        sns.pairplot(df, hue ='class_numeric')\n",
    "        plt.show()\n",
    "        sns.histplot(data=df, x=\"class_numeric\")\n",
    "        plt.show()    \n",
    "        return()\n",
    "        \n",
    "x=Klasi(X,y)\n",
    "#x.plotdata()   \n",
    "X_train, X_test, y_train, y_test = x.splitdata()\n",
    "X_train = scale.fit_transform(X_train) \n",
    "X_test = scale.transform(X_test) \n",
    "\n",
    "svc = svm.SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "yhat=svc.predict(X_test)\n",
    "#X_test_original1 = scale.inverse_transform(X_test)\n",
    "\n",
    "\n",
    "k=accuracy_score(y_test, yhat,normalize=True) ##!!prosoxi ta orismata \n",
    "print(\"Accuracy score:\",k)\n",
    "\n",
    "##Ex.1.3\n",
    "\n",
    "#GridSearchCV  #Use it to tune the C hyperparameter AND the kernel hyperparameter (and any other hyperparameter\n",
    "#you wish) of the SVM applied on your data.\n",
    "\n",
    "parameters = {'kernel':('linear', 'poly', 'rbf'), 'C':[1, 5, 10]}\n",
    "\n",
    "clf = GridSearchCV(svc, parameters,cv=5)\n",
    "clf.fit(X_train,y_train)\n",
    "print(\"Best score:\", clf.best_score_)\n",
    "print(\"Best parameters:\", clf.best_params_)\n",
    "\n",
    "# evaluate the performance of your best model\n",
    "#2o erwrthma ex1.3 xrisimopoiw ta test dedomena \n",
    "\n",
    "best_model = svm.SVC( C=1,kernel='rbf')\n",
    "best_model=best_model.fit(X_train,y_train)\n",
    "yhattest=best_model.predict(X_test)\n",
    "best_acc=accuracy_score(y_test,yhattest)\n",
    "print(\"Accuracy score for the best model:\",best_acc)\n",
    "\n",
    "#Ex.1.4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b377e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405162ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
